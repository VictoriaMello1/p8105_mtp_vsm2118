---
title: "Data Science Midterm"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuious.color = "viridis",
  ggplot2.continuious.fill = "viridis"
  
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

The raw data used in this analysis consists of Change of Address (COA) data from the US Postal Service for New York City between 2018 and 2022. The COA data includes information on the total number of permanent address changes going into and out of each ZIP code in New York City. The initial section of the report focuses on importing, cleaning, and ensuring data quality. This process results in a clean and structured dataset, ready for analysis. The dataset's characteristics, including observations, unique ZIP codes, and neighborhoods, are described. Data quality issues are also addressed. The subsequent section visualizes the data with an analysis of address change trends, uncovering extreme values, and plots that highlight neighborhood-level insights over the five-year period. The report concludes with an acknowledgment of any limitations of the dataset.

## Section 1 â€“ Data import, cleaning, and quality control

```{r}
zip_data = read_csv("zip_codes.csv")

sheet_names <- c("2018", "2019", "2020", "2021", "2022")
coa_data <- map_dfr(sheet_names, ~ readxl::read_xlsx("USPS.xlsx", sheet = .x))

view(coa_data)

coa_data = coa_data %>% 
  janitor::clean_names() %>% 
  mutate(
    year = as.integer(str_sub(month, 1, 4)),
    net_change = `total_perm_in` - `total_perm_out`
  )
  
view(coa_data)
```

Cleaning zipcode data and creating a borough variable

```{r}
# Create a "borough" variable in the ZIP code data
zip_data <- zip_data %>%
  janitor::clean_names() %>% 
  rename(zipcode = zip_code) %>% 
  mutate(
    borough = case_when(
      county_name %in% c("Kings") ~ "Brooklyn",
      county_name %in% c("Queens") ~ "Queens",
      county_name == "Bronx" ~ "Bronx",
      county_name == "New York" ~ "Manhattan",
      county_name == "Richmond" ~ "Staten Island"
    )
  )
view(zip_data)
```

Merging the zipcode and coa data sets

```{r}
# Merge the COA and ZIP code data based on ZIP code
merged_data <- coa_data %>%
  left_join(zip_data, by = "zipcode")

# Select only the necessary variables for later visualizations
final_data <- merged_data %>%
  select(zipcode, neighborhood, borough, year, month, net_change, city)

view(final_data)
```

Exploring combined final dataset:
```{r}
summary_description <- final_data %>%
  summarise(
    Total_Observations = n(),
    Unique_ZIP_Codes = n_distinct(zipcode),
    Unique_Neighborhoods = n_distinct(neighborhood)
  )

print(summary_description)
```

Comparing city and borough: 
```{r}
# Filter the data for Manhattan and Queens
manhattan_data <- final_data %>%
  filter(borough == "Manhattan")

queens_data <- final_data %>%
  filter(borough == "Queens")

# Create tables for the most common cities in Manhattan and Queens
manhattan_table <- manhattan_data %>%
  count(city, sort = TRUE)

queens_table <- queens_data %>%
  count(city, sort = TRUE)

knitr::kable(manhattan_table, caption = "Manhattan most common city value")
knitr::kable(queens_table, caption = "Queens most common city value")
```

